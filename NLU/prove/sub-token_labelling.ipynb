{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ettore/.conda/envs/nlu/lib/python3.10/site-packages/sklearn/utils/__init__.py:16: UserWarning: A NumPy version >=1.22.4 and <1.29.0 is required for this version of SciPy (detected version 1.22.3)\n",
      "  from scipy.sparse import issparse\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    dataset = []\n",
    "    with open(path, \"r\") as file:\n",
    "        dataset = json.loads(file.read())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def split_sets(tmp_train_raw):\n",
    "    portion = 0.10\n",
    "\n",
    "    intents = [x[\"intent\"] for x in tmp_train_raw]  # We stratify on intents\n",
    "    count_y = Counter(intents)\n",
    "\n",
    "    labels = []\n",
    "    inputs = []\n",
    "    mini_train = []\n",
    "\n",
    "    for id_y, y in enumerate(intents):\n",
    "        if count_y[y] > 1:  # If some intents occurs only once, we put them in training\n",
    "            inputs.append(tmp_train_raw[id_y])\n",
    "            labels.append(y)\n",
    "        else:\n",
    "            mini_train.append(tmp_train_raw[id_y])\n",
    "\n",
    "    # Random Stratify\n",
    "    X_train, X_dev, _, _ = train_test_split(\n",
    "        inputs,\n",
    "        labels,\n",
    "        test_size=portion,\n",
    "        random_state=42,\n",
    "        shuffle=True,\n",
    "        stratify=labels,\n",
    "    )\n",
    "    X_train.extend(mini_train)\n",
    "    train_raw = X_train\n",
    "    dev_raw = X_dev\n",
    "\n",
    "    return train_raw, dev_raw\n",
    "\n",
    "def get_data(\n",
    "    train=os.path.join(\"../dataset\", \"ATIS\", \"train.json\"),\n",
    "    test=os.path.join(\"../dataset\", \"ATIS\", \"test.json\"),\n",
    "):\n",
    "\n",
    "    tmp_train_raw = load_data(train)\n",
    "    test_raw = load_data(test)\n",
    "\n",
    "    train_raw, dev_raw = split_sets(tmp_train_raw)\n",
    "\n",
    "    logging.info(\"Train size: %d\", len(train_raw))\n",
    "    logging.info(\"Dev size: %d\", len(dev_raw))\n",
    "    logging.info(\"Test size: %d\", len(test_raw))\n",
    "\n",
    "    return train_raw, dev_raw, test_raw\n",
    "\n",
    "train_raw, dev_raw, test_raw = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify train_row, dev_raw and test_raw\n",
    "So that it can be tokenized and put well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "slots_set = set()\n",
    "intents_set = set()\n",
    "\n",
    "for phrases in [train_raw, dev_raw, test_raw]:\n",
    "    for phrase in phrases:\n",
    "        for slot in phrase[\"slots\"].split():\n",
    "            slots_set.add(slot)\n",
    "        intents_set.add(phrase[\"intent\"])\n",
    "\n",
    "slots2id = {\"pad\": 0}\n",
    "id2slots = {0: \"O\"}\n",
    "for slot in (slots_set):\n",
    "    slots2id[slot] = len(slots2id)\n",
    "    id2slots[len(id2slots)] = slot\n",
    "\n",
    "intent2id = {}\n",
    "id2intent = {}\n",
    "for intent in (intents_set):\n",
    "    intent2id[intent] = len(intent2id)\n",
    "    id2intent[len(id2intent)] = intent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    text_labels = text_labels.split()\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        # labels.extend([label] + [\"X\"] * (n_subwords - 1))\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "def tokenize_data(raw_data):\n",
    "    processed_data = []\n",
    "    for dset in raw_data:\n",
    "        tokenized_set = {}\n",
    "        tokenized_set[\"raw_intent\"] = dset[\"intent\"]\n",
    "        tokenized_set[\"raw_slots\"] = dset[\"slots\"]\n",
    "        tokenized_set[\"raw_utterance\"] = dset[\"utterance\"]\n",
    "\n",
    "        tokenized_sentence, adapted_labels = tokenize_and_preserve_labels(\n",
    "            dset[\"utterance\"], dset[\"slots\"]\n",
    "        )\n",
    "\n",
    "        tokenized_set[\"tokenized_utterance\"] = tokenized_sentence\n",
    "        tokenized_set[\"tokenized_slots\"] = adapted_labels\n",
    "\n",
    "        processed_data.append(tokenized_set)\n",
    "    return processed_data\n",
    "\n",
    "def encode_data(tokenized_data):\n",
    "    encoded_data = []\n",
    "    for dset in tokenized_data:\n",
    "        encoded_set = {}\n",
    "        \n",
    "        encoded_set[\"raw_intent\"] = dset[\"raw_intent\"]\n",
    "        encoded_set[\"raw_slots\"] = dset[\"raw_slots\"]\n",
    "        encoded_set[\"raw_utterance\"] = dset[\"raw_utterance\"]\n",
    "        encoded_set[\"tokenized_utterance\"] = dset[\"tokenized_utterance\"]\n",
    "        encoded_set[\"tokenized_slots\"] = dset[\"tokenized_slots\"]\n",
    "\n",
    "        # Encode the tokenized utterance\n",
    "        encoded_set[\"encoded_utterance\"] = tokenizer.encode_plus(dset[\"tokenized_utterance\"], add_special_tokens=False)\n",
    "\n",
    "        # Encode the tokenized slots\n",
    "        encoded_set[\"encoded_slots\"] = [slots2id[slot] for slot in dset[\"tokenized_slots\"]]\n",
    "\n",
    "        # Encode the intent\n",
    "        encoded_set[\"encoded_intent\"] = intent2id[dset[\"raw_intent\"]]\n",
    "\n",
    "        encoded_data.append(encoded_set)\n",
    "    return encoded_data\n",
    "\n",
    "def preprocess_data(raw_data):\n",
    "    # Tokenize `utterance` and `slots` with sub-token labelling. The subtoken is labelled with `X`.\n",
    "    # The `X` label is used to indicate that the subtoken is not the first subtoken of a word.\n",
    "    processed_data = tokenize_data(raw_data)\n",
    "\n",
    "    # Encode the tokenized data\n",
    "    encoded_data = encode_data(processed_data)\n",
    "\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "err = 0\n",
    "for tokenized_data in processed_train:\n",
    "    if ((len(tokenized_data[\"encoded_utterance\"][\"input_ids\"]) - len(tokenized_data[\"encoded_slots\"])) != 0):\n",
    "        err += 1\n",
    "\n",
    "        print(tokenized_data[\"tokenized_utterance\"])\n",
    "        print(tokenized_data[\"tokenized_slots\"])\n",
    "        print(tokenized_data[\"encoded_utterance\"])\n",
    "        print(tokenized_data[\"encoded_slots\"])\n",
    "        print(tokenized_data[\"encoded_intent\"])\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "        break\n",
    "\n",
    "print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_train[0][\"tokenized_utterance\"]))\n",
    "print(len(processed_train[0][\"tokenized_slots\"]))\n",
    "print(len(processed_train[0][\"encoded_utterance\"][\"input_ids\"]))\n",
    "print(len(processed_train[0][\"encoded_slots\"]))\n",
    "print((processed_train[0][\"encoded_intent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_intent': 'airfare',\n",
       " 'raw_slots': 'O O O O O O O O B-fromloc.city_name O B-toloc.city_name',\n",
       " 'raw_utterance': 'what is the cost for these flights from baltimore to philadelphia',\n",
       " 'tokenized_utterance': ['what',\n",
       "  'is',\n",
       "  'the',\n",
       "  'cost',\n",
       "  'for',\n",
       "  'these',\n",
       "  'flights',\n",
       "  'from',\n",
       "  'baltimore',\n",
       "  'to',\n",
       "  'philadelphia'],\n",
       " 'tokenized_slots': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-fromloc.city_name',\n",
       "  'O',\n",
       "  'B-toloc.city_name'],\n",
       " 'encoded_utterance': {'input_ids': [2054, 2003, 1996, 3465, 2005, 2122, 7599, 2013, 6222, 2000, 4407], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " 'encoded_slots': [12, 12, 12, 12, 12, 12, 12, 12, 102, 12, 14],\n",
       " 'encoded_intent': 12}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "1. Create new torch.data.Dataset child with the new data\n",
    "2. Create custom collate_fn for the `DataLoader`\n",
    "\n",
    "Question: where to move data from to GPU? The easiest solution would be in the `__getitem__` of the `Dataset`, but this is not optimal (non batched operation, it probably requires gpu sync). If it's possible to do it inside the collate fn then it would be awesome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ATISDataset(Dataset):\n",
    "    def __init__ (self, processed_data):\n",
    "        self.input = [data[\"encoded_utterance\"][\"input_ids\"] for data in processed_data]\n",
    "        self.attention_mask = [data[\"encoded_utterance\"][\"attention_mask\"] for data in processed_data]\n",
    "        self.token_type_ids = [data[\"encoded_utterance\"][\"token_type_ids\"] for data in processed_data]\n",
    "        self.slots = [data[\"encoded_slots\"] for data in processed_data]\n",
    "        self.intent = [data[\"encoded_intent\"] for data in processed_data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input[idx], self.attention_mask[idx], self.token_type_ids[idx], self.slots[idx], self.intent[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2054, 2003, 1996, 3465, 2005, 2122, 7599, 2013, 6222, 2000, 4407],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [12, 12, 12, 12, 12, 12, 12, 12, 102, 12, 14],\n",
       " 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to yourself:\n",
    "\n",
    "if you add some padding, it's zeroes everywhere.\n",
    "\n",
    "([2054, 2003, 1996, 3465, 2005, 2122, 7599, 2013, 6222, 2000, 4407, 0, 0],  \n",
    " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],  \n",
    " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  \n",
    " [12, 12, 12, 12, 12, 12, 12, 12, 55, 12, 91, 0, 0],  \n",
    " 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_rn(data):\n",
    "    device = get_device()\n",
    "\n",
    "    # Get the max length of the input sequence\n",
    "    max_len = max([len(sentence) for sentence, _, _, _, _ in data])\n",
    "\n",
    "    # PAD all the input sequences to the max length\n",
    "    slots_len = torch.tensor([len(slots) for _, _, _, slots, _ in data]).to(device)\n",
    "    input_ids = torch.tensor([sentence + [0] * (max_len - len(sentence)) for sentence, _, _, _, _ in data]).to(device)\n",
    "    attention_mask = torch.tensor([[1] * len(mask) + [0] * (max_len - len(mask)) for _, mask, _, _, _ in data]).to(device)\n",
    "    token_type_ids = torch.tensor([token_type_ids + [0] * (max_len - len(token_type_ids)) for _, _, token_type_ids, _, _ in data]).to(device)\n",
    "    slots = torch.tensor([slots + [0] * (max_len - len(slots)) for _, _, _, slots, _ in data]).to(device)\n",
    "    intent = torch.tensor([intent for _, _, _, _, intent in data]).to(device)\n",
    "\n",
    "    return {\n",
    "        \"slots_len\": slots_len,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"slots\": slots,\n",
    "        \"intent\": intent\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ATISDataset(processed_train)\n",
    "test_dataset = ATISDataset(processed_train)\n",
    "dev_dataset = ATISDataset(processed_train)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_rn)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=64, shuffle=False, collate_fn=collate_rn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 24])\n",
      "torch.Size([64, 24])\n",
      "torch.Size([64, 24])\n",
      "torch.Size([64, 24])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for dataloader in train_dataloader:\n",
    "    print(dataloader[\"input_ids\"].shape)\n",
    "    print(dataloader[\"attention_mask\"].shape)\n",
    "    print(dataloader[\"token_type_ids\"].shape)\n",
    "    print(dataloader[\"slots\"].shape)\n",
    "    print(dataloader[\"intent\"].shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to feed the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class IntentSlotModel(torch.nn.Module):\n",
    "    def __init__(self, slot_len, intent_len):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.intent_classifier = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, intent_len\n",
    "        )\n",
    "        self.slot_classifier = torch.nn.Linear(self.bert.config.hidden_size, slot_len)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "\n",
    "        return intent_logits, slot_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from conll import evaluate\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def calculate_loss(\n",
    "    intent_loss_fn, slot_loss_fn, intent_logits, slot_logits, intent_labels, slot_labels\n",
    "):\n",
    "    intent_loss = intent_loss_fn(intent_logits, intent_labels)\n",
    "    # intent_loss = 0\n",
    "    slot_loss = slot_loss_fn(slot_logits.view(-1, len(slots2id)), slot_labels.view(-1))\n",
    "    # slot_loss = 0\n",
    "    return intent_loss + slot_loss\n",
    "\n",
    "def eval_loop(\n",
    "    model: IntentSlotModel,\n",
    "    dataloader,\n",
    "    intent_loss_fn,\n",
    "    slot_loss_fn,\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "\n",
    "    ref_intents = []\n",
    "    hyp_intents = []\n",
    "\n",
    "    ref_slots = []\n",
    "    hyp_slots = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            # input_ids, attention_mask, intent_labels, slot_labels = data\n",
    "            # intent_labels = intent_labels.squeeze(1)\n",
    "\n",
    "            # Forward\n",
    "            intent_logits, slot_logits = model(data[\"input_ids\"], data[\"attention_mask\"], data[\"token_type_ids\"])\n",
    "            total_loss.append(\n",
    "                calculate_loss(\n",
    "                    intent_loss_fn=intent_loss_fn,\n",
    "                    intent_logits=intent_logits,\n",
    "                    intent_labels=data[\"intent\"],\n",
    "                    slot_loss_fn=slot_loss_fn,\n",
    "                    slot_logits=slot_logits,\n",
    "                    slot_labels=data[\"slots\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            intent_hyp = torch.argmax(intent_logits, dim=1)\n",
    "            slot_hyp = torch.argmax(slot_logits, dim=2)\n",
    "\n",
    "            # Intent inference\n",
    "            ref_intents.extend(data[\"intent\"].to(\"cpu\").tolist())\n",
    "            hyp_intents.extend(intent_hyp.to(\"cpu\").tolist())\n",
    "                    \n",
    "            # Slot filling inference\n",
    "            input_ids = data[\"input_ids\"].to(\"cpu\").tolist()\n",
    "            if data[\"slots\"].shape != slot_hyp.shape and data[\"slots\"].shape != input_ids.shape:\n",
    "                print(\"Shape mismatch\")\n",
    "                print(data[\"slots\"].shape)\n",
    "                print(slot_hyp.shape)\n",
    "                print(input_ids.shape)\n",
    "                exit()\n",
    "\n",
    "            for input, s_ref, s_hyp, seq_length in zip(input_ids, data[\"slots\"], slot_hyp, data[\"slots_len\"]):\n",
    "                tmp_ref = []\n",
    "                tmp_hyp = []\n",
    "\n",
    "                utterance = tokenizer.tokenize(tokenizer.decode(input, include_special_tokens=False))[:seq_length]\n",
    "\n",
    "                for u, r,h in zip(utterance, s_ref, s_hyp):\n",
    "                    tmp_ref.append((u, f\"{id2slots[r.item()]}\"))\n",
    "                    tmp_hyp.append((u, f\"{id2slots[h.item()]}\"))\n",
    "\n",
    "                ref_slots.append(tmp_ref)\n",
    "                hyp_slots.append(tmp_hyp)\n",
    "        \n",
    "        f1_slot = evaluate(ref_slots, hyp_slots)\n",
    "\n",
    "        accuracy_intention = classification_report(\n",
    "                ref_intents,\n",
    "                hyp_intents,\n",
    "                output_dict=True,\n",
    "                zero_division=False,\n",
    "            )['accuracy']\n",
    "\n",
    "        # print(accuracy_intention)\n",
    "        # print(f1_slot[\"total\"][\"f\"])\n",
    "        return accuracy_intention, f1_slot[\"total\"][\"f\"], total_loss\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model: IntentSlotModel,\n",
    "    data,\n",
    "    optimizer,\n",
    "    intent_loss_fn,\n",
    "    slot_loss_fn,\n",
    "    scheduler\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    # input_ids, attention_mask, token_type_ids, slot_labels, intent_labels = data\n",
    "    # intent_labels = intent_labels.squeeze(1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    intent_logits, slot_logits = model(data[\"input_ids\"], data[\"attention_mask\"], data[\"token_type_ids\"])\n",
    "\n",
    "    loss = calculate_loss(\n",
    "        intent_loss_fn=intent_loss_fn,\n",
    "        intent_logits=intent_logits,\n",
    "        intent_labels=data[\"intent\"],\n",
    "        slot_loss_fn=slot_loss_fn,\n",
    "        slot_logits=slot_logits,\n",
    "        slot_labels=data[\"slots\"],\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "model = IntentSlotModel(len(slots2id), len(intent2id))\n",
    "model.to(get_device())\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "intent_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "slot_loss_fn = torch.nn.CrossEntropyLoss(ignore_index=slots2id[\"pad\"])\n",
    "\n",
    "epochs = 6\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "warmup_steps = int(0.1*total_steps)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=len(train_dataloader) * epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs | Acc: 0.9987 - F1: 0.9801 - Loss: 0.0432: 100%|██████████| 6/6 [01:53<00:00, 18.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9987 - F1: 0.9801 - Loss: 0.0432\n"
     ]
    }
   ],
   "source": [
    "accuracy, f1, loss = 0, 0, [float(\"inf\")]\n",
    "bloss = 0\n",
    "\n",
    "# Create the outer tqdm progress bar for epochs\n",
    "epochs_tqdm = tqdm(range(epochs), desc=f\"Epochs | Acc: {accuracy:.4f} - F1: {f1:.4f} - Loss: {sum(loss)/len(loss):.4f}\")\n",
    "\n",
    "for epoch in epochs_tqdm:\n",
    "    # Create the inner tqdm progress bar for batches\n",
    "    batch_tqdm = tqdm(enumerate(train_dataloader), desc=f\"Batch | Loss: {bloss:.4f}\", leave=False)\n",
    "\n",
    "    for index, batch in batch_tqdm:\n",
    "        bloss = train_loop(model, batch, optimizer, intent_loss_fn, slot_loss_fn, scheduler)\n",
    "        batch_tqdm.set_description(f\"Batch | Loss: {bloss:.4f}\")\n",
    "        \n",
    "    accuracy, f1, loss = eval_loop(model, dev_dataloader, intent_loss_fn, slot_loss_fn)\n",
    "    epochs_tqdm.set_description(f\"Epochs | Acc: {accuracy:.4f} - F1: {f1:.4f} - Loss: {sum(loss)/len(loss):.4f}\")\n",
    "\n",
    "accuracy, f1, loss = eval_loop(model, test_dataloader, intent_loss_fn, slot_loss_fn)\n",
    "print(f\"Accuracy: {accuracy:.4f} - F1: {f1:.4f} - Loss: {sum(loss)/len(loss):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9929 - F1: 0.9295 - Loss: 0.1703\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    bloss = train_loop(model, batch, optimizer, intent_loss_fn, slot_loss_fn, scheduler)\n",
    "        \n",
    "accuracy, f1, loss = eval_loop(model, dev_dataloader, intent_loss_fn, slot_loss_fn)\n",
    "print(f\"Accuracy: {accuracy:.4f} - F1: {f1:.4f} - Loss: {sum(loss)/len(loss):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3473973959.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[26], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Accuracy: 0.9911 - F1: 0.9454 - Loss: 0.1569\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Accuracy: 0.9911 - F1: 0.9454 - Loss: 0.1569"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
