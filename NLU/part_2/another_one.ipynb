{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ettore/.conda/envs/nlu/lib/python3.10/site-packages/sklearn/utils/__init__.py:16: UserWarning: A NumPy version >=1.22.4 and <1.29.0 is required for this version of SciPy (detected version 1.22.3)\n",
      "  from scipy.sparse import issparse\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4978\n",
      "Test samples: 893\n",
      "TRAIN size: 4480\n",
      "DEV size: 498\n",
      "TEST size: 893\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    '''\n",
    "        input: path/to/data\n",
    "        output: json \n",
    "    '''\n",
    "    dataset = []\n",
    "    with open(path) as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    return dataset\n",
    "\n",
    "tmp_train_raw = load_data(os.path.join('../dataset','ATIS','train.json'))\n",
    "test_raw = load_data(os.path.join('../dataset','ATIS','test.json'))\n",
    "print('Train samples:', len(tmp_train_raw))\n",
    "print('Test samples:', len(test_raw))\n",
    "# pprint(tmp_train_raw[0])\n",
    "\n",
    "portion = 0.10\n",
    "\n",
    "intents = [x['intent'] for x in tmp_train_raw] # We stratify on intents\n",
    "count_y = Counter(intents)\n",
    "\n",
    "labels = []\n",
    "inputs = []\n",
    "mini_train = []\n",
    "\n",
    "for id_y, y in enumerate(intents):\n",
    "    if count_y[y] > 1: # If some intents occurs only once, we put them in training\n",
    "        inputs.append(tmp_train_raw[id_y])\n",
    "        labels.append(y)\n",
    "    else:\n",
    "        mini_train.append(tmp_train_raw[id_y])\n",
    "# Random Stratify\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(inputs, labels, test_size=portion, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=labels)\n",
    "X_train.extend(mini_train)\n",
    "train_raw = X_train\n",
    "dev_raw = X_dev\n",
    "\n",
    "y_test = [x['intent'] for x in test_raw]\n",
    "\n",
    "# # Intent distributions\n",
    "# print('Train:')\n",
    "# pprint({k:round(v/len(y_train),3)*100 for k, v in sorted(Counter(y_train).items())})\n",
    "# print('Dev:'), \n",
    "# pprint({k:round(v/len(y_dev),3)*100 for k, v in sorted(Counter(y_dev).items())})\n",
    "# print('Test:') \n",
    "# pprint({k:round(v/len(y_test),3)*100 for k, v in sorted(Counter(y_test).items())})\n",
    "# print('='*89)\n",
    "# Dataset size\n",
    "print('TRAIN size:', len(train_raw))\n",
    "print('DEV size:', len(dev_raw))\n",
    "print('TEST size:', len(test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Words: 864\n",
      "# Slots: 129\n",
      "# Intent: 26\n"
     ]
    }
   ],
   "source": [
    "PAD_TOKEN = 0\n",
    "\n",
    "# Count the number of unique words, slot and intent tags\n",
    "words_set = set()\n",
    "slot_set = set()\n",
    "intent_set = set()\n",
    "\n",
    "for example in train_raw:\n",
    "    for word in example['utterance'].split():\n",
    "        if word not in words_set:\n",
    "            words_set.add(word)\n",
    "    for slot in example['slots'].split():\n",
    "        if slot not in slot_set:\n",
    "            slot_set.add(slot)\n",
    "    if example['intent'] not in intent_set:\n",
    "        intent_set.add(example['intent'])\n",
    "        \n",
    "for example in dev_raw:\n",
    "    # for word in example['utterance'].split():\n",
    "    #     if word not in words_set:\n",
    "    #         words_set.add(word)\n",
    "    for slot in example['slots'].split():\n",
    "        if slot not in slot_set:\n",
    "            slot_set.add(slot)\n",
    "    if example['intent'] not in intent_set:\n",
    "        intent_set.add(example['intent'])\n",
    "        \n",
    "for example in test_raw:\n",
    "    # for word in example['utterance'].split():\n",
    "    #     if word not in words_set:\n",
    "    #         words_set.add(word)\n",
    "    for slot in example['slots'].split():\n",
    "        if slot not in slot_set:\n",
    "            slot_set.add(slot)\n",
    "    if example['intent'] not in intent_set:\n",
    "        intent_set.add(example['intent'])\n",
    "\n",
    "print('# Words:', len(words_set))\n",
    "print('# Slots:', len(slot_set))\n",
    "print('# Intent:', len(intent_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# this is a simple way to fix the issue of BERT breaking words into subwords\n",
    "# so the issue of sub-tokenization\n",
    "# as suggested here: https://stackoverflow.com/questions/62082938/how-to-stop-bert-from-breaking-apart-specific-words-into-word-piece\n",
    "# tokenizer.add_tokens(list(slot_set))\n",
    "\n",
    "# Example dataset\n",
    "rdata = [\n",
    "    {\n",
    "        \"utterance\": \"what is the cost for these flights from baltimore to philadelphia\",\n",
    "        \"slots\": \"O O O O O O O O B-fromloc.city_name O B-toloc.city_name\",\n",
    "        \"intent\": \"airfare\",\n",
    "    },\n",
    "    {\n",
    "        \"utterance\": \"flights from westchester county to san francisco daily\",\n",
    "        \"slots\": \"O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name I-toloc.city_name B-flight_days\",\n",
    "        \"intent\": \"flight\",\n",
    "    },\n",
    "    {\n",
    "        \"utterance\": \"i would like a flight from philadelphia to dallas on american airlines\",\n",
    "        \"slots\": \"O O O O O O B-fromloc.city_name O B-toloc.city_name O B-airline_name I-airline_name\",\n",
    "        \"intent\": \"flight\",\n",
    "    },\n",
    "    {\n",
    "        \"utterance\": \"can i take a single airline from la to charlotte to newark back to la\",\n",
    "        \"slots\": \"O O O O O O O B-fromloc.city_name O B-toloc.city_name O B-toloc.city_name O O B-fromloc.city_name\",\n",
    "        \"intent\": \"airline\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "utterance = [t[\"utterance\"] for t in rdata]\n",
    "slots = [t[\"slots\"] for t in rdata]\n",
    "intent = [t[\"intent\"] for t in rdata]\n",
    "\n",
    "tokenized_utterance = tokenizer(utterance, return_tensors='pt', truncation=True, padding='max_length', max_length=64)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "output = model(**tokenized_utterance)\n",
    "\n",
    "# var.pooler_output # intent\n",
    "# var.last_hidden_state # slot\n",
    "\n",
    "# tokenizer.convert_ids_to_tokens(torch.argmax(var.last_hidden_state, dim=1).tolist()[0])\n",
    "\n",
    "\n",
    "# print(\"Input tokens: \", tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0])))\n",
    "# print(\"Intent labels untokenized: \", tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(intent_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.pooler_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 768])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor(tokenizer(intent).input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.convert_ids_to_tokens([101, 1051, 1051, 1051, 1051, 1051, 1051, 1051, 1051, 1038, 1011, 2013, 4135, 2278, 1012, 2103, 1035, 2171, 1051, 1038, 1011, 2000, 4135, 2278, 1012, 2103, 1035, 2171, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# # tokenizer.add_tokens(list(slot_set))\n",
    "\n",
    "# tokenized_utterance = tokenizer(utterance, return_tensors='pt', truncation=True, padding='max_length', max_length=64)\n",
    "# tokenizer.convert_ids_to_tokens(tokenized_utterance['input_ids'][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
